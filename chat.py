# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RBrUgGNnXSheRY0uuXqNGqX0uU7z2rvg
"""

!pip install transformers datasets evaluate


import json
from datasets import Dataset

from google.colab import drive
drive.mount('/content/drive')

import torch
from transformers import BertForQuestionAnswering, BertTokenizer, Trainer, TrainingArguments
from datasets import Dataset
import evaluate



# Load your custom dataset
squad_path = "/content/drive/MyDrive/good_dataset.json"  # Replace with the correct path
squad_dataset = Dataset.from_json(squad_path)

# Split into training and validation sets
squad_dataset = squad_dataset.train_test_split(test_size=0.1)
train_dataset = squad_dataset["train"]
val_dataset = squad_dataset["test"]

import json

with open("/content/drive/MyDrive/good_dataset.json", "r") as f:
    data = json.load(f)

print(json.dumps(data, indent=4)[:1000])  # Print the first 1000 characters for inspection

from transformers import BertTokenizerFast

# Load the BERT tokenizer
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

def preprocess_function(examples):
    # Initialize lists to hold tokenized input and target positions
    tokenized_inputs = {
        "input_ids": [],
        "attention_mask": [],
        "start_positions": [],
        "end_positions": []
    }

    # Loop through data, paragraphs, and questions
    for entry in examples["data"]:
        for paragraph in entry["paragraphs"]:
            context = paragraph["context"]
            for qa in paragraph["qas"]:
                question = qa["question"]
                answers = qa["answers"]
                answer_text = answers[0]["text"]
                answer_start = answers[0]["answer_start"]

                # Tokenize question and context together
                tokenized = tokenizer(
                    question,
                    context,
                    truncation=True,
                    padding="max_length",
                    max_length=384,
                    return_offsets_mapping=True,
                )

                # Get offset mapping
                offsets = tokenized.pop("offset_mapping")

                # Find start and end positions for the answer
                start_position = None
                end_position = None
                for idx, (start, end) in enumerate(offsets):
                    if start <= answer_start < end:
                        start_position = idx
                    if start < answer_start + len(answer_text) <= end:
                        end_position = idx

                # Append data if valid positions are found
                if start_position is not None and end_position is not None:
                    tokenized_inputs["input_ids"].append(tokenized["input_ids"])
                    tokenized_inputs["attention_mask"].append(tokenized["attention_mask"])
                    tokenized_inputs["start_positions"].append(start_position)
                    tokenized_inputs["end_positions"].append(end_position)

    return tokenized_inputs

# Apply the preprocessing function to train and validation datasets
tokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=["data"])
tokenized_val = val_dataset.map(preprocess_function, batched=True, remove_columns=["data"])



from transformers import BertForQuestionAnswering

model = BertForQuestionAnswering.from_pretrained("bert-base-uncased")

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="steps",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=500,
    save_steps=500,
    eval_steps=500,
    load_best_model_at_end=True,
    save_total_limit=2,
     report_to="none",  # Disable logging to W&B
)

# Load the SQuAD metric
metric = evaluate.load("squad")

def compute_metrics(pred):
    start_predictions, end_predictions = pred.predictions
    start_labels = pred.label_ids["start_positions"]
    end_labels = pred.label_ids["end_positions"]

    # Compute predictions and references
    predictions = [
        {
            "id": str(i),
            "prediction_text": tokenizer.decode(start_predictions[i:end_predictions[i]], skip_special_tokens=True)
        }
        for i in range(len(start_predictions))
    ]
    references = [
        {
            "id": str(i),
            "answers": {"text": [tokenizer.decode(start_labels[i:end_labels[i]], skip_special_tokens=True)]}
        }
        for i in range(len(start_labels))
    ]

    # Return the computed metrics
    return metric.compute(predictions=predictions, references=references)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

import numpy as np

def compute_metrics(pred):
    # Extract start and end logits from predictions
    start_logits, end_logits = pred.predictions

    # Extract ground-truth start and end positions from label_ids
    labels = pred.label_ids
    start_labels = labels[:, 0]  # Ground-truth start positions
    end_labels = labels[:, 1]    # Ground-truth end positions

    # Compute predicted start and end positions
    start_predictions = np.argmax(start_logits, axis=1)
    end_predictions = np.argmax(end_logits, axis=1)

    # Compute accuracy for start and end positions
    start_accuracy = (start_predictions == start_labels).mean()
    end_accuracy = (end_predictions == end_labels).mean()

    # Return metrics
    return {
        "start_accuracy": start_accuracy,
        "end_accuracy": end_accuracy,
        "overall_accuracy": (start_accuracy + end_accuracy) / 2
    }

import os
os.environ["WANDB_DISABLED"] = "true"

trainer.train()
# metrics = trainer.evaluate()
# print(metrics)
model.save_pretrained("/content/drive/MyDrive/fine_tuned_bert")
tokenizer.save_pretrained("/content/drive/MyDrive/fine_tuned_bert")

from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

# Load your fine-tuned BERT model
model_path = "/content/drive/MyDrive/fine_tuned_bert"  # Replace with your model path
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForQuestionAnswering.from_pretrained(model_path)

import json

# Load the predefined context file
with open('/content/drive/MyDrive/preloaded_contexts.json', 'r') as file:
    preloaded_contexts = json.load(file)

from sentence_transformers import SentenceTransformer, util

# Load a pre-trained sentence embedding model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Precompute embeddings for all questions in the predefined context file
question_texts = [entry['question'] for entry in preloaded_contexts]
question_embeddings = embedding_model.encode(question_texts)

def retrieve_context(user_query, preloaded_contexts, question_embeddings):
    # Encode the user query
    query_embedding = embedding_model.encode(user_query)

    # Compute similarity scores
    similarities = util.cos_sim(query_embedding, question_embeddings)

    # Find the best match
    best_match_idx = similarities.argmax().item()
    return preloaded_contexts[best_match_idx]['context']

def get_answer(user_query, context):
    """
    Use the fine-tuned BERT model to answer the user's query based on the given context.
    """
    # Tokenize input
    inputs = tokenizer.encode_plus(user_query, context, return_tensors="pt")

    # Get model predictions
    with torch.no_grad():
        outputs = model(**inputs)
        start_idx = torch.argmax(outputs.start_logits)
        end_idx = torch.argmax(outputs.end_logits) + 1

    # Decode the answer
    answer = tokenizer.decode(inputs['input_ids'][0][start_idx:end_idx])
    return answer

def chatbot():
    print("Welcome to the Customer Support Chatbot!")
    print("Type 'exit' to end the chat.\n")

    while True:
        # Get user input
        user_query = input("You: ")
        if user_query.lower() == "exit":
            print("Chatbot: Goodbye! Have a great day!")
            break

        # Retrieve the most relevant context
        context = retrieve_context(user_query, preloaded_contexts, question_embeddings)

        if context:
            # Get the answer from the fine-tuned model
            answer = get_answer(user_query, context)
            print(f"Chatbot: {answer}")
        else:
            print("Chatbot: Sorry, I couldn't find relevant information. Please try again.")

chatbot()
