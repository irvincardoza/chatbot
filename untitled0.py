# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WQ-l7jMrOmTCZ5Z-7iYq2ldrQiE2LieB
"""

!pip install transformers datasets evaluate


import json
from datasets import Dataset

from google.colab import drive
drive.mount('/content/drive')

import torch
from transformers import BertForQuestionAnswering, BertTokenizer, Trainer, TrainingArguments
from datasets import Dataset
import evaluate



# Load your custom dataset
squad_path = "/content/drive/MyDrive/good_dataset.json"  # Replace with the correct path
squad_dataset = Dataset.from_json(squad_path)

# Split into training and validation sets
squad_dataset = squad_dataset.train_test_split(test_size=0.1)
train_dataset = squad_dataset["train"]
val_dataset = squad_dataset["test"]

import json

with open("/content/drive/MyDrive/good_dataset.json", "r") as f:
    data = json.load(f)

print(json.dumps(data, indent=4)[:1000])  # Print the first 1000 characters for inspection

from transformers import BertTokenizerFast

# Load the BERT tokenizer
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

def preprocess_function(examples):
    # Initialize lists to hold tokenized input and target positions
    tokenized_inputs = {
        "input_ids": [],
        "attention_mask": [],
        "start_positions": [],
        "end_positions": []
    }

    # Loop through data, paragraphs, and questions
    for entry in examples["data"]:
        for paragraph in entry["paragraphs"]:
            context = paragraph["context"]
            for qa in paragraph["qas"]:
                question = qa["question"]
                answers = qa["answers"]
                answer_text = answers[0]["text"]
                answer_start = answers[0]["answer_start"]

                # Tokenize question and context together
                tokenized = tokenizer(
                    question,
                    context,
                    truncation=True,
                    padding="max_length",
                    max_length=384,
                    return_offsets_mapping=True,
                )

                # Get offset mapping
                offsets = tokenized.pop("offset_mapping")

                # Find start and end positions for the answer
                start_position = None
                end_position = None
                for idx, (start, end) in enumerate(offsets):
                    if start <= answer_start < end:
                        start_position = idx
                    if start < answer_start + len(answer_text) <= end:
                        end_position = idx

                # Append data if valid positions are found
                if start_position is not None and end_position is not None:
                    tokenized_inputs["input_ids"].append(tokenized["input_ids"])
                    tokenized_inputs["attention_mask"].append(tokenized["attention_mask"])
                    tokenized_inputs["start_positions"].append(start_position)
                    tokenized_inputs["end_positions"].append(end_position)

    return tokenized_inputs

# Apply the preprocessing function to train and validation datasets
tokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=["data"])
tokenized_val = val_dataset.map(preprocess_function, batched=True, remove_columns=["data"])



from transformers import BertForQuestionAnswering

model = BertForQuestionAnswering.from_pretrained("bert-base-uncased")

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="steps",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=500,
    save_steps=500,
    eval_steps=500,
    load_best_model_at_end=True,
    save_total_limit=2,
     report_to="none",  # Disable logging to W&B
)

# Load the SQuAD metric
metric = evaluate.load("squad")

def compute_metrics(pred):
    start_predictions, end_predictions = pred.predictions
    start_labels = pred.label_ids["start_positions"]
    end_labels = pred.label_ids["end_positions"]

    # Compute predictions and references
    predictions = [
        {
            "id": str(i),
            "prediction_text": tokenizer.decode(start_predictions[i:end_predictions[i]], skip_special_tokens=True)
        }
        for i in range(len(start_predictions))
    ]
    references = [
        {
            "id": str(i),
            "answers": {"text": [tokenizer.decode(start_labels[i:end_labels[i]], skip_special_tokens=True)]}
        }
        for i in range(len(start_labels))
    ]

    # Return the computed metrics
    return metric.compute(predictions=predictions, references=references)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

import numpy as np

def compute_metrics(pred):
    # Extract start and end logits from predictions
    start_logits, end_logits = pred.predictions

    # Extract ground-truth start and end positions from label_ids
    labels = pred.label_ids
    start_labels = labels[:, 0]  # Ground-truth start positions
    end_labels = labels[:, 1]    # Ground-truth end positions

    # Compute predicted start and end positions
    start_predictions = np.argmax(start_logits, axis=1)
    end_predictions = np.argmax(end_logits, axis=1)

    # Compute accuracy for start and end positions
    start_accuracy = (start_predictions == start_labels).mean()
    end_accuracy = (end_predictions == end_labels).mean()

    # Return metrics
    return {
        "start_accuracy": start_accuracy,
        "end_accuracy": end_accuracy,
        "overall_accuracy": (start_accuracy + end_accuracy) / 2
    }

import os
os.environ["WANDB_DISABLED"] = "true"

trainer.train()
# metrics = trainer.evaluate()
# print(metrics)
model.save_pretrained("/content/drive/MyDrive/fine_tuned_bert")
tokenizer.save_pretrained("/content/drive/MyDrive/fine_tuned_bert")

from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch
import json

# Load the fine-tuned model and tokenizer
model_path = "/content/drive/MyDrive/fine_tuned_bert"  # Update this to your path
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForQuestionAnswering.from_pretrained(model_path)

# Load your JSON dataset for reference (optional, for fallback testing)
# with open('/mnt/data/good_dataset.json', 'r') as f:
#     dataset = json.load(f)

# Function to query the model
def ask_question(question):
    # Use a static or generalized context if no explicit context is required


    # Tokenize the input (question and generalized context)
    inputs = tokenizer.encode_plus(
        question,

        return_tensors="pt",
        truncation=True,
        max_length=512
    )

    # Predict the answer
    with torch.no_grad():
        outputs = model(**inputs)
        start_idx = torch.argmax(outputs.start_logits)
        end_idx = torch.argmax(outputs.end_logits) + 1

    # Decode the answer
    answer = tokenizer.decode(inputs["input_ids"][0][start_idx:end_idx])
    return answer

# Interactive chat function
def chat_with_model():
    print("=== Q&A Chat with Fine-Tuned BERT ===")
    print("Type 'exit' to quit.\n")

    while True:
        question = input("User: ")
        if question.lower() == "exit":
            print("Exiting chat. Goodbye!")
            break

        # Get the answer from the model
        answer = ask_question(question)
        print(f"Model: {answer}\n")

# Run the interactive chat
chat_with_model()

